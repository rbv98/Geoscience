{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53d2757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import csv\n",
    "\n",
    "print(\"âœ… Libraries loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1c57d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Config set.\n"
     ]
    }
   ],
   "source": [
    "features = ['gr','dt', 'dt_nct', 'rhob_combined', 'res_deep', 'hp', 'ob']\n",
    "target_col = 'ppp'\n",
    "depth_col = 'depth'\n",
    "\n",
    "files = ['QAZIAN -1X.CSV', 'MISSA KESWAL-01.CSV', 'MISSA KESWAL-03.CSV']\n",
    "print(\"âœ… Config set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd724d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded and cleaned QAZIAN -1X.CSV\n",
      "âœ… Loaded and cleaned MISSA KESWAL-01.CSV\n",
      "âœ… Loaded and cleaned MISSA KESWAL-03.CSV\n",
      "âœ… All files merged.\n"
     ]
    }
   ],
   "source": [
    "def detect_delimiter(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        sample = f.readline()\n",
    "        sniffer = csv.Sniffer()\n",
    "        return sniffer.sniff(sample).delimiter\n",
    "\n",
    "data_frames = []\n",
    "for f in files:\n",
    "    try:\n",
    "        delim = detect_delimiter(f)\n",
    "        df = pd.read_csv(f, delimiter=delim, engine='python')\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        df.replace(-999.25, np.nan, inplace=True)\n",
    "        df = df[features + [target_col, depth_col]]\n",
    "        df.dropna(inplace=True)\n",
    "        data_frames.append(df)\n",
    "        print(f\"âœ… Loaded and cleaned {f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {f}: {e}\")\n",
    "\n",
    "df = pd.concat(data_frames, ignore_index=True)\n",
    "print(\"âœ… All files merged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32a25651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data split and scaled.\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_df[features])\n",
    "y_train = train_df[target_col].values\n",
    "X_test = scaler.transform(test_df[features])\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "print(\"âœ… Data split and scaled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f714d344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Callbacks set.\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6)\n",
    "print(\"âœ… Callbacks set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84b33a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7024827.0000 - val_loss: 714048.5625 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 554876.3750 - val_loss: 348834.6875 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 475395.4062 - val_loss: 300302.8438 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 452771.4688 - val_loss: 299920.2500 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 437083.4688 - val_loss: 281642.7812 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 415455.7500 - val_loss: 272954.3125 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 421041.4688 - val_loss: 296243.3438 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 399144.8750 - val_loss: 276562.0312 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 406023.9062 - val_loss: 267759.3125 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 401667.1875 - val_loss: 256129.3750 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 398586.7500 - val_loss: 255513.6250 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 380772.5625 - val_loss: 269944.6875 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 385604.2500 - val_loss: 250909.8281 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 366936.9375 - val_loss: 238576.3438 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 368263.5938 - val_loss: 245372.2031 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 363847.8750 - val_loss: 253397.8750 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 368815.9688 - val_loss: 233331.2969 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 354270.9688 - val_loss: 233658.0156 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 358245.1250 - val_loss: 227133.8906 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 346243.3438 - val_loss: 247559.3750 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 357013.0938 - val_loss: 244664.8906 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 358561.4688 - val_loss: 224044.3125 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 354156.4375 - val_loss: 224694.0938 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 352680.6875 - val_loss: 223753.3281 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 353907.6562 - val_loss: 217207.2031 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 353180.6875 - val_loss: 241011.0000 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 328706.8438 - val_loss: 238339.4688 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 336060.8125 - val_loss: 212520.1562 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 328533.5938 - val_loss: 216489.4062 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 331005.6875 - val_loss: 247054.3438 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 342351.7500 - val_loss: 219611.0000 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 333456.3750 - val_loss: 212943.0469 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 333442.8438 - val_loss: 206537.7656 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 331907.3125 - val_loss: 214706.3125 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 339341.5312 - val_loss: 229289.2812 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 325239.1250 - val_loss: 245271.3281 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 323588.7500 - val_loss: 209556.1719 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 333461.2188 - val_loss: 206103.5469 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 343691.5312 - val_loss: 202056.8750 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 315649.4062 - val_loss: 218181.7812 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 324140.9062 - val_loss: 223368.5469 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 331486.7188 - val_loss: 214167.3281 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 330919.1562 - val_loss: 228311.7656 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 321426.5312 - val_loss: 204825.9531 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 314663.1250 - val_loss: 208295.7344 - learning_rate: 5.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 314925.0625 - val_loss: 202707.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 305468.5938 - val_loss: 212802.5625 - learning_rate: 5.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 306329.3750 - val_loss: 200260.0312 - learning_rate: 5.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 319631.1875 - val_loss: 198996.8906 - learning_rate: 5.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 315693.6875 - val_loss: 205761.7812 - learning_rate: 5.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 318994.5938 - val_loss: 237959.1562 - learning_rate: 5.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 302450.8438 - val_loss: 199503.7500 - learning_rate: 5.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 297091.4688 - val_loss: 187671.8750 - learning_rate: 5.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 309377.6875 - val_loss: 206750.5469 - learning_rate: 5.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 314661.1250 - val_loss: 194414.9219 - learning_rate: 5.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 314577.7500 - val_loss: 190059.1875 - learning_rate: 5.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 312098.9688 - val_loss: 197908.1094 - learning_rate: 5.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 300503.4062 - val_loss: 215407.8281 - learning_rate: 5.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 307503.7812 - val_loss: 210302.6406 - learning_rate: 2.5000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 305572.1250 - val_loss: 212286.6719 - learning_rate: 2.5000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 298972.1875 - val_loss: 197936.0156 - learning_rate: 2.5000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 299190.2500 - val_loss: 196449.8281 - learning_rate: 2.5000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 296058.5625 - val_loss: 228395.6875 - learning_rate: 2.5000e-04\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step\n",
      "âœ… CNN model trained.\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential([\n",
    "    Input(shape=(X_train_cnn.shape[1], 1)),\n",
    "    Conv1D(64, kernel_size=2, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(128, kernel_size=2, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn.compile(optimizer='adam', loss='mse')\n",
    "cnn.fit(X_train_cnn, y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr], verbose=1)\n",
    "y_pred_cnn = cnn.predict(X_test_cnn).flatten()\n",
    "print(\"âœ… CNN model trained.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57ddf9c1-95f0-44e5-8a05-0dd369c7b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CNN model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to your preferred directory\n",
    "cnn.save('/Users/IDEAPAD/Documents/Research work/Geoscience project/Well Data/PPCNN/cnn_model_nosphi.h5')\n",
    "print(\"âœ… CNN model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0886c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/IDEAPAD/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 9951386.0000 - val_loss: 9890385.0000 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 9838328.0000 - val_loss: 9518340.0000 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 9315401.0000 - val_loss: 8792960.0000 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 8686660.0000 - val_loss: 8201782.0000 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 7966494.0000 - val_loss: 7395968.0000 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 7137414.0000 - val_loss: 6494188.0000 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 6244447.5000 - val_loss: 5706906.0000 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 5480107.5000 - val_loss: 4990289.0000 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 4721507.0000 - val_loss: 4147588.0000 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 4012123.5000 - val_loss: 3319638.0000 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 3225238.7500 - val_loss: 2832840.2500 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 2588744.0000 - val_loss: 2126846.2500 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 2000679.6250 - val_loss: 1700999.6250 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 1508503.0000 - val_loss: 1184929.0000 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 1120951.8750 - val_loss: 897622.6250 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 817217.3750 - val_loss: 583100.1250 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 580743.8125 - val_loss: 482775.8750 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 450109.7812 - val_loss: 363151.0938 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 364021.8125 - val_loss: 304406.7812 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 319201.9688 - val_loss: 275043.0938 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 305263.7812 - val_loss: 212987.8125 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 303493.2188 - val_loss: 222577.1719 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 289654.0312 - val_loss: 272376.7500 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 281529.4688 - val_loss: 255407.9688 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 288450.3125 - val_loss: 225433.7500 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 285334.6875 - val_loss: 205900.4531 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 271468.9375 - val_loss: 254776.0312 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 284074.4375 - val_loss: 202397.8750 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 276797.0312 - val_loss: 245328.3594 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 269730.5625 - val_loss: 213204.4844 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 272433.5938 - val_loss: 244204.3750 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 269753.8125 - val_loss: 233191.1250 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 268493.7812 - val_loss: 214558.7500 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 272548.5625 - val_loss: 215215.9062 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 260657.4844 - val_loss: 191448.9531 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 277566.4062 - val_loss: 192891.0469 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 266677.2188 - val_loss: 201158.4062 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 258753.5312 - val_loss: 196280.3594 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 265384.8125 - val_loss: 197431.2500 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 264909.5000 - val_loss: 230604.6406 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 259216.2656 - val_loss: 191192.9062 - learning_rate: 2.5000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 257729.5625 - val_loss: 252459.6250 - learning_rate: 2.5000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 267476.8438 - val_loss: 256455.1406 - learning_rate: 2.5000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 259909.4219 - val_loss: 240345.9531 - learning_rate: 2.5000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 258480.1406 - val_loss: 195220.4844 - learning_rate: 2.5000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 255871.5625 - val_loss: 250862.6719 - learning_rate: 2.5000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 255628.4375 - val_loss: 229392.7812 - learning_rate: 1.2500e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 264159.0312 - val_loss: 221938.2656 - learning_rate: 1.2500e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 247818.1875 - val_loss: 195035.9688 - learning_rate: 1.2500e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 253578.6562 - val_loss: 187898.4844 - learning_rate: 1.2500e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 252871.9844 - val_loss: 241917.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 259139.7500 - val_loss: 240535.0469 - learning_rate: 1.2500e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 251564.3750 - val_loss: 226154.5625 - learning_rate: 1.2500e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 238326.6094 - val_loss: 187577.0781 - learning_rate: 1.2500e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 258970.8281 - val_loss: 183412.0469 - learning_rate: 1.2500e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 251914.1562 - val_loss: 208842.0781 - learning_rate: 1.2500e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 256567.1250 - val_loss: 267669.0312 - learning_rate: 1.2500e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 257912.0938 - val_loss: 255083.1250 - learning_rate: 1.2500e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 255932.2969 - val_loss: 189405.2031 - learning_rate: 1.2500e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 255669.5938 - val_loss: 224473.1719 - learning_rate: 1.2500e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 255170.8438 - val_loss: 244196.3594 - learning_rate: 6.2500e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 253440.0781 - val_loss: 186010.2188 - learning_rate: 6.2500e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 253073.3594 - val_loss: 184638.9688 - learning_rate: 6.2500e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 256319.4219 - val_loss: 243778.4531 - learning_rate: 6.2500e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m342/342\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 255562.9219 - val_loss: 223381.6719 - learning_rate: 6.2500e-05\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step\n",
      "âœ… DFNN training and prediction complete.\n"
     ]
    }
   ],
   "source": [
    "# dfnn = Sequential([\n",
    "#     Input(shape=(X_train.shape[1],)),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "# dfnn.compile(optimizer='adam', loss='mse')\n",
    "# dfnn.fit(X_train, y_train, validation_split=0.3, epochs=100, batch_size=32,\n",
    "#          callbacks=[early_stop, reduce_lr], verbose=1)\n",
    "# y_pred_dfnn = dfnn.predict(X_test).flatten()\n",
    "# print(\"âœ… DFNN model trained.\")\n",
    "\n",
    "\n",
    "# ğŸ”§ Tunable parameters (can be changed for performance tuning)\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units_1 = 128   # Number of neurons in first hidden layer\n",
    "hidden_units_2 = 64    # Number of neurons in second hidden layer\n",
    "dropout_rate_1 = 0.3   # Dropout after first hidden layer\n",
    "dropout_rate_2 = 0.3   # Dropout after second hidden layer\n",
    "epochs = 100           # Number of training iterations\n",
    "batch_size = 32        # Batch size for training\n",
    "\n",
    "# Define DFNN model\n",
    "dfnn = Sequential([\n",
    "    Dense(hidden_units_1, activation='relu', input_shape=(input_dim,)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dropout_rate_1),\n",
    "    \n",
    "    Dense(hidden_units_2, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dropout_rate_2),\n",
    "    \n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "dfnn.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "dfnn.fit(X_train, y_train,\n",
    "         validation_split=0.3,\n",
    "         epochs=epochs,\n",
    "         batch_size=batch_size,\n",
    "         callbacks=[early_stop, reduce_lr],\n",
    "         verbose=1)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_dfnn = dfnn.predict(X_test).flatten()\n",
    "\n",
    "print(\"âœ… DFNN training and prediction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "848f81a8-741a-4a4d-9f31-ed6247d823b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DFNN model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to your preferred directory\n",
    "dfnn.save('/Users/IDEAPAD/Documents/Research work/Geoscience project/Well Data/PPCNN/dfnn_model_nosphi.h5')\n",
    "print(\"âœ… DFNN model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d29b5899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CNN:\n",
      "   RÂ²     : 0.7885\n",
      "   MSE    : 192656.02\n",
      "   RMSE   : 438.93\n",
      "   MAE    : 312.46\n",
      "   Rel RMSE: 8.32% of PPP range\n",
      "\n",
      "ğŸ“Š DFNN:\n",
      "   RÂ²     : 0.7988\n",
      "   MSE    : 183316.73\n",
      "   RMSE   : 428.16\n",
      "   MAE    : 308.07\n",
      "   Rel RMSE: 8.11% of PPP range\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(name, y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rel_rmse = (rmse / (max(y_true) - min(y_true))) * 100\n",
    "    print(f\"ğŸ“Š {name}:\")\n",
    "    print(f\"   RÂ²     : {r2:.4f}\")\n",
    "    print(f\"   MSE    : {mse:.2f}\")\n",
    "    print(f\"   RMSE   : {rmse:.2f}\")\n",
    "    print(f\"   MAE    : {mae:.2f}\")\n",
    "    print(f\"   Rel RMSE: {rel_rmse:.2f}% of PPP range\\n\")\n",
    "\n",
    "evaluate_model(\"CNN\", y_test, y_pred_cnn)\n",
    "evaluate_model(\"DFNN\", y_test, y_pred_dfnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5245a42-f9f7-436d-a22b-fa0fad729d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
