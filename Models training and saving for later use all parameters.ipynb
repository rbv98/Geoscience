{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d2757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/IDEAPAD/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import csv\n",
    "\n",
    "print(\"âœ… Libraries loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1c57d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Config set.\n"
     ]
    }
   ],
   "source": [
    "features = ['gr','dt', 'dt_nct', 'rhob_combined', 'res_deep', 'sphi', 'hp', 'ob']\n",
    "target_col = 'ppp'\n",
    "depth_col = 'depth'\n",
    "\n",
    "files = ['QAZIAN -1X.CSV', 'MISSA KESWAL-01.CSV', 'MISSA KESWAL-03.CSV']\n",
    "print(\"âœ… Config set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd724d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded and cleaned QAZIAN -1X.CSV\n",
      "âœ… Loaded and cleaned MISSA KESWAL-01.CSV\n",
      "âœ… Loaded and cleaned MISSA KESWAL-03.CSV\n",
      "âœ… All files merged.\n"
     ]
    }
   ],
   "source": [
    "def detect_delimiter(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        sample = f.readline()\n",
    "        sniffer = csv.Sniffer()\n",
    "        return sniffer.sniff(sample).delimiter\n",
    "\n",
    "data_frames = []\n",
    "for f in files:\n",
    "    try:\n",
    "        delim = detect_delimiter(f)\n",
    "        df = pd.read_csv(f, delimiter=delim, engine='python')\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        df.replace(-999.25, np.nan, inplace=True)\n",
    "        df = df[features + [target_col, depth_col]]\n",
    "        df.dropna(inplace=True)\n",
    "        data_frames.append(df)\n",
    "        print(f\"âœ… Loaded and cleaned {f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {f}: {e}\")\n",
    "\n",
    "df = pd.concat(data_frames, ignore_index=True)\n",
    "print(\"âœ… All files merged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a25651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data split and scaled.\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_df[features])\n",
    "y_train = train_df[target_col].values\n",
    "X_test = scaler.transform(test_df[features])\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "print(\"âœ… Data split and scaled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f714d344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Callbacks set.\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6)\n",
    "print(\"âœ… Callbacks set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b33a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7220710.0000 - val_loss: 1717481.5000 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 456294.5938 - val_loss: 315250.1562 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 326405.0938 - val_loss: 182332.9844 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 324701.1250 - val_loss: 164984.9844 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 292771.1875 - val_loss: 165510.8906 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 287459.0938 - val_loss: 155191.8906 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 277504.5000 - val_loss: 155841.2500 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 281191.4062 - val_loss: 154694.7812 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 275802.1250 - val_loss: 145393.5312 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 273468.8125 - val_loss: 134431.0781 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 264859.2188 - val_loss: 135407.2031 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 266534.3750 - val_loss: 136414.2188 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 256481.5938 - val_loss: 140042.6875 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 249733.5781 - val_loss: 157699.2656 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 249121.9219 - val_loss: 125187.6172 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 256058.8125 - val_loss: 122970.3438 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 264279.5312 - val_loss: 122870.3906 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 239502.2656 - val_loss: 132922.7500 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 244113.6562 - val_loss: 120187.2656 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 241109.2188 - val_loss: 134239.3438 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 231073.8125 - val_loss: 126699.3594 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 244928.7188 - val_loss: 120318.1172 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 239589.3281 - val_loss: 125749.2656 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 232253.6719 - val_loss: 124956.8438 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 238729.9062 - val_loss: 121063.2422 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 243954.4062 - val_loss: 119040.7266 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 228984.8125 - val_loss: 123540.4922 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 231837.6094 - val_loss: 118560.3828 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 222135.3281 - val_loss: 119608.4141 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 220912.0312 - val_loss: 122953.7578 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 235644.7500 - val_loss: 112238.3594 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 223607.4531 - val_loss: 117111.0234 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 224910.6875 - val_loss: 118118.0625 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 234407.1875 - val_loss: 121000.8516 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 225907.4219 - val_loss: 121978.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 213487.7344 - val_loss: 127921.3359 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 222572.2031 - val_loss: 115018.3906 - learning_rate: 2.5000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 229428.7188 - val_loss: 114241.1719 - learning_rate: 2.5000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 217127.4375 - val_loss: 118193.8047 - learning_rate: 2.5000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 234851.1406 - val_loss: 118756.3203 - learning_rate: 2.5000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 220257.0625 - val_loss: 111817.3438 - learning_rate: 2.5000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 224391.3125 - val_loss: 112824.9375 - learning_rate: 2.5000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 217117.3750 - val_loss: 114740.6172 - learning_rate: 2.5000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 216225.4062 - val_loss: 115035.8203 - learning_rate: 2.5000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 219235.0781 - val_loss: 113541.3125 - learning_rate: 2.5000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 224157.6094 - val_loss: 109218.8125 - learning_rate: 2.5000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 217004.9531 - val_loss: 115566.7109 - learning_rate: 2.5000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 227404.5312 - val_loss: 115693.3047 - learning_rate: 2.5000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 217982.1094 - val_loss: 118738.8594 - learning_rate: 2.5000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 225863.7656 - val_loss: 114695.7578 - learning_rate: 2.5000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 220911.9062 - val_loss: 116458.8203 - learning_rate: 2.5000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 211110.7188 - val_loss: 109944.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 208291.8125 - val_loss: 114256.3594 - learning_rate: 1.2500e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 222216.5781 - val_loss: 117460.5781 - learning_rate: 1.2500e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 216886.0625 - val_loss: 111467.7812 - learning_rate: 1.2500e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m259/259\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 217924.5156 - val_loss: 114262.1172 - learning_rate: 1.2500e-04\n",
      "\u001b[1m139/139\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step\n",
      "âœ… CNN model trained.\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential([\n",
    "    Input(shape=(X_train_cnn.shape[1], 1)),\n",
    "    Conv1D(64, kernel_size=2, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(128, kernel_size=2, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn.compile(optimizer='adam', loss='mse')\n",
    "cnn.fit(X_train_cnn, y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr], verbose=1)\n",
    "y_pred_cnn = cnn.predict(X_test_cnn).flatten()\n",
    "print(\"âœ… CNN model trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ddf9c1-95f0-44e5-8a05-0dd369c7b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CNN model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to your preferred directory\n",
    "cnn.save('/Users/IDEAPAD/Documents/Research work/Geoscience project/Well Data/PPCNN/cnn_model.h5')\n",
    "print(\"âœ… CNN model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0886c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/IDEAPAD/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8833658.0000 - val_loss: 8577747.0000 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 8814681.0000 - val_loss: 8395779.0000 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 8578485.0000 - val_loss: 8099934.5000 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 8302709.0000 - val_loss: 7757114.0000 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7785275.0000 - val_loss: 7329316.0000 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 7419655.5000 - val_loss: 6931929.5000 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 6967361.0000 - val_loss: 6394048.0000 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 6438979.0000 - val_loss: 5904133.5000 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 6064226.0000 - val_loss: 5348314.0000 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 5503528.0000 - val_loss: 4930254.0000 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 4920359.0000 - val_loss: 4374126.0000 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 4529459.5000 - val_loss: 3867226.7500 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 4092626.7500 - val_loss: 3512602.5000 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 3584753.2500 - val_loss: 3040413.5000 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3083230.2500 - val_loss: 2499507.2500 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 2715125.0000 - val_loss: 2187684.7500 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 2271267.7500 - val_loss: 1860105.1250 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 1948199.3750 - val_loss: 1500962.0000 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 1625497.1250 - val_loss: 1187047.1250 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 1297185.6250 - val_loss: 908687.0625 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 1021608.3125 - val_loss: 728728.2500 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 844325.1250 - val_loss: 537756.8750 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 671374.3750 - val_loss: 432889.8750 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 562241.1875 - val_loss: 282588.7812 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 417323.7812 - val_loss: 239293.1094 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 355334.1875 - val_loss: 161100.2656 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 276709.5625 - val_loss: 131513.3125 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 226622.7188 - val_loss: 114167.1094 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 227418.8750 - val_loss: 107589.4688 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 218210.4688 - val_loss: 95095.5234 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 197964.5469 - val_loss: 97089.1094 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 204446.6406 - val_loss: 99953.8828 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 194805.4219 - val_loss: 97518.1328 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 206880.7969 - val_loss: 100720.8359 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 189117.8281 - val_loss: 97966.6719 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 196596.1406 - val_loss: 97929.8359 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 187313.9531 - val_loss: 95438.2812 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 183878.9688 - val_loss: 91066.6328 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 186953.5000 - val_loss: 100834.1250 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 184733.7500 - val_loss: 93358.4922 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 192822.3125 - val_loss: 99591.6328 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 191926.2969 - val_loss: 94569.8672 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 189721.1562 - val_loss: 97911.5000 - learning_rate: 5.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 185765.0938 - val_loss: 100848.4766 - learning_rate: 2.5000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 182966.6406 - val_loss: 95851.0469 - learning_rate: 2.5000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 186239.1719 - val_loss: 91773.7969 - learning_rate: 2.5000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 174268.5938 - val_loss: 88724.6875 - learning_rate: 2.5000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 183145.5312 - val_loss: 87726.0625 - learning_rate: 2.5000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - loss: 180455.9531 - val_loss: 93457.2969 - learning_rate: 2.5000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 183115.4375 - val_loss: 99426.6719 - learning_rate: 2.5000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 186693.1250 - val_loss: 93568.5234 - learning_rate: 2.5000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 181102.6250 - val_loss: 91406.7422 - learning_rate: 2.5000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 180356.5625 - val_loss: 97393.8984 - learning_rate: 2.5000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 182067.5469 - val_loss: 87830.2812 - learning_rate: 1.2500e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 186476.0781 - val_loss: 93221.8516 - learning_rate: 1.2500e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 175953.8750 - val_loss: 92630.4062 - learning_rate: 1.2500e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 188390.2188 - val_loss: 97364.8828 - learning_rate: 1.2500e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m227/227\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 181750.4219 - val_loss: 89413.4922 - learning_rate: 1.2500e-04\n",
      "\u001b[1m139/139\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step\n",
      "âœ… DFNN training and prediction complete.\n"
     ]
    }
   ],
   "source": [
    "# dfnn = Sequential([\n",
    "#     Input(shape=(X_train.shape[1],)),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "# dfnn.compile(optimizer='adam', loss='mse')\n",
    "# dfnn.fit(X_train, y_train, validation_split=0.3, epochs=100, batch_size=32,\n",
    "#          callbacks=[early_stop, reduce_lr], verbose=1)\n",
    "# y_pred_dfnn = dfnn.predict(X_test).flatten()\n",
    "# print(\"âœ… DFNN model trained.\")\n",
    "\n",
    "# ğŸ”§ Tunable parameters (can be changed for performance tuning)\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units_1 = 128   # Number of neurons in first hidden layer\n",
    "hidden_units_2 = 64    # Number of neurons in second hidden layer\n",
    "dropout_rate_1 = 0.3   # Dropout after first hidden layer\n",
    "dropout_rate_2 = 0.3   # Dropout after second hidden layer\n",
    "epochs = 100           # Number of training iterations\n",
    "batch_size = 32        # Batch size for training\n",
    "\n",
    "# Define DFNN model\n",
    "dfnn = Sequential([\n",
    "    Dense(hidden_units_1, activation='relu', input_shape=(input_dim,)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dropout_rate_1),\n",
    "    \n",
    "    Dense(hidden_units_2, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dropout_rate_2),\n",
    "    \n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "dfnn.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "dfnn.fit(X_train, y_train,\n",
    "         validation_split=0.3,\n",
    "         epochs=epochs,\n",
    "         batch_size=batch_size,\n",
    "         callbacks=[early_stop, reduce_lr],\n",
    "         verbose=1)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_dfnn = dfnn.predict(X_test).flatten()\n",
    "\n",
    "print(\"âœ… DFNN training and prediction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848f81a8-741a-4a4d-9f31-ed6247d823b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DFNN model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to your preferred directory\n",
    "dfnn.save('/Users/IDEAPAD/Documents/Research work/Geoscience project/Well Data/PPCNN/dfnn_model.h5')\n",
    "print(\"âœ… DFNN model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29b5899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CNN:\n",
      "   RÂ²     : 0.8687\n",
      "   MSE    : 108739.40\n",
      "   RMSE   : 329.76\n",
      "   MAE    : 243.41\n",
      "   Rel RMSE: 6.36% of PPP range\n",
      "\n",
      "ğŸ“Š DFNN:\n",
      "   RÂ²     : 0.9004\n",
      "   MSE    : 82459.23\n",
      "   RMSE   : 287.16\n",
      "   MAE    : 200.43\n",
      "   Rel RMSE: 5.54% of PPP range\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(name, y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rel_rmse = (rmse / (max(y_true) - min(y_true))) * 100\n",
    "    print(f\"ğŸ“Š {name}:\")\n",
    "    print(f\"   RÂ²     : {r2:.4f}\")\n",
    "    print(f\"   MSE    : {mse:.2f}\")\n",
    "    print(f\"   RMSE   : {rmse:.2f}\")\n",
    "    print(f\"   MAE    : {mae:.2f}\")\n",
    "    print(f\"   Rel RMSE: {rel_rmse:.2f}% of PPP range\\n\")\n",
    "\n",
    "evaluate_model(\"CNN\", y_test, y_pred_cnn)\n",
    "evaluate_model(\"DFNN\", y_test, y_pred_dfnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5245a42-f9f7-436d-a22b-fa0fad729d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
