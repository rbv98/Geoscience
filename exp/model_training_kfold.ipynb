{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb2c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 01:38:45.453510: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-01 01:38:47.009149: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-01 01:38:53.235193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model, clone_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, MaxPooling1D,\n",
    "    Dropout, Flatten, Dense, LSTM, GRU\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarslyStopping, ReduceLROnPlateau\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, anderson_ksamp\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ad7e0b-e65e-4aab-86d5-bc4f87dcc848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d54f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tvd',\n",
       " 'dt',\n",
       " 'dt_nct',\n",
       " 'gr',\n",
       " 'sphi',\n",
       " 'hp',\n",
       " 'ob',\n",
       " 'rhob_combined',\n",
       " 'res_deep',\n",
       " 'eaton_ratio',\n",
       " 'hp_gradient',\n",
       " 'ob_gradient',\n",
       " 'tvd_normalized']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('preprocessing_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "features = metadata['predictor_features']\n",
    "\n",
    "target_col = 'ppp'\n",
    "depth_col = 'tvd'\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a88ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data sizes:\n",
      "  Train+Val combined: 188631 (for CV stacking)\n",
      "  Test: 84973 (for final evaluation)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_data.csv')\n",
    "val_df = pd.read_csv('val_data.csv')\n",
    "train_val_combined = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "blind_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "X_train_val = train_val_combined[features].values\n",
    "y_train_val = train_val_combined[target_col].values\n",
    "\n",
    "X_test = blind_df[features].values\n",
    "y_test = blind_df[target_col].values\n",
    "\n",
    "print(f\"\\nData sizes:\")\n",
    "print(f\"  Train+Val combined: {len(X_train_val)} (for CV stacking)\")\n",
    "print(f\"  Test: {len(X_test)} (for final evaluation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645a7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21061b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Performing 5-fold cross-validation...\n",
      "\n",
      "Fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764571138.283071  676035 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 140842 MB memory:  -> device: 0, name: NVIDIA H200, pci bus id: 0000:4c:00.0, compute capability: 9.0\n",
      "2025-12-01 01:39:02.134117: I external/local_xla/xla/service/service.cc:163] XLA service 0x146f80014190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-01 01:39:02.134134: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA H200, Compute Capability 9.0\n",
      "2025-12-01 01:39:02.220231: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-01 01:39:02.554640: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91400\n",
      "2025-12-01 01:39:02.782843: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:39:02.782870: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:39:02.782900: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:39:02.782913: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:39:02.782928: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:39:03.628359: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1580', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:05.159782: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2069', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:05.770765: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2069', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:06.733444: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2868', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:08.146975: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2868', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "I0000 00:00:1764571152.575556  676174 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-12-01 01:39:21.340702: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:39:22.805697: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:23.723748: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:39:23.723773: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:39:24.243843: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:24.499972: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:24.946424: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:25.377341: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:26.014867: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-12-01 01:39:26.222603: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_166', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:03.307020: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:41:03.307047: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:41:03.910426: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:04.411033: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:05.095969: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:05.497441: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:05.984401: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:06.054905: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:08.094463: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:41:08.094512: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:41:08.094530: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:41:08.580339: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_722', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:09.971968: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1808', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:10.308662: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_722', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:11.842578: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1901', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:14.207994: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_select_fusion', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:19.261049: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 01:41:20.381321: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:20.625159: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-12-01 01:41:20.829638: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 02:06:32.706693: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 02:06:32.706719: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 02:06:32.910975: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-12-01 02:06:33.473135: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 02:06:33.876747: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-01 02:06:34.920658: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-12-01 02:06:35.396443: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 02:06:35.410624: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-12-01 02:06:38.570861: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_select_fusion', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 02:18:22.329268: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_select_fusion', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 02:32:44.668939: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_select_fusion', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 02:49:24.668525: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_select_fusion', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Out-of-fold predictions generated!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for out-of-fold predictions\n",
    "oof_preds = {\n",
    "    'CNN': np.zeros(len(X_train_val)),\n",
    "    'DFNN': np.zeros(len(X_train_val)),\n",
    "    'RNN': np.zeros(len(X_train_val)),\n",
    "    'RF': np.zeros(len(X_train_val)),\n",
    "    'XGBoost': np.zeros(len(X_train_val))\n",
    "}\n",
    "\n",
    "# Storage for final models (trained on all data)\n",
    "final_models = {}\n",
    "\n",
    "print(f\"\\nPerforming {n_folds}-fold cross-validation...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_fold_train = X_train_val[train_idx]\n",
    "    y_fold_train = y_train_val[train_idx]\n",
    "    X_fold_val = X_train_val[val_idx]\n",
    "    y_fold_val = y_train_val[val_idx]\n",
    "    \n",
    "    # Scale\n",
    "    fold_scaler = StandardScaler()\n",
    "    X_fold_train_s = fold_scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_s = fold_scaler.transform(X_fold_val)\n",
    "    \n",
    "    # Reshape for CNN/RNN\n",
    "    X_fold_train_cnn = X_fold_train_s.reshape(-1, len(features), 1)\n",
    "    X_fold_val_cnn = X_fold_val_s.reshape(-1, len(features), 1)\n",
    "    X_fold_train_rnn = X_fold_train_s.reshape(-1, len(features), 1)\n",
    "    X_fold_val_rnn = X_fold_val_s.reshape(-1, len(features), 1)\n",
    "    \n",
    "    # Train CNN\n",
    "    cnn_fold = Sequential([\n",
    "        Input((len(features),1)),\n",
    "        Conv1D(64, kernel_size=2, padding='same', activation='relu',\n",
    "               kernel_regularizer=regularizers.L2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        Conv1D(128, kernel_size=2, padding='same', activation='relu',\n",
    "               kernel_regularizer=regularizers.L2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.002)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.L2(0.002)),\n",
    "        BatchNormalization(), \n",
    "        Dropout(0.3),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    cnn_fold.compile(optimizer='adam', loss='mse')\n",
    "    cnn_fold.fit(X_fold_train_cnn, y_fold_train,\n",
    "                 validation_split=0.1, epochs=100, batch_size=32,\n",
    "                 callbacks=[es, rlr], verbose=0)\n",
    "    oof_preds['CNN'][val_idx] = cnn_fold.predict(X_fold_val_cnn, verbose=0).flatten()\n",
    "    \n",
    "    # Train DFNN\n",
    "    dfnn_fold = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(len(features),)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.15),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.15),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    dfnn_fold.compile(optimizer='adam', loss='mse')\n",
    "    dfnn_fold.fit(X_fold_train_s, y_fold_train,\n",
    "                  validation_split=0.1, epochs=100, batch_size=64,\n",
    "                  callbacks=[es, rlr], verbose=0)\n",
    "    oof_preds['DFNN'][val_idx] = dfnn_fold.predict(X_fold_val_s, verbose=0).flatten()\n",
    "    \n",
    "    # Train RNN\n",
    "    rnn_fold = Sequential([\n",
    "        Input((len(features), 1)),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    rnn_fold.compile(optimizer='adam', loss='mse')\n",
    "    rnn_fold.fit(X_fold_train_rnn, y_fold_train,\n",
    "                 validation_split=0.1, epochs=130, batch_size=16,\n",
    "                 callbacks=[es, rlr], verbose=0)\n",
    "    oof_preds['RNN'][val_idx] = rnn_fold.predict(X_fold_val_rnn, verbose=0).flatten()\n",
    "    \n",
    "    # Train RF\n",
    "    rf_fold = RandomForestRegressor(\n",
    "        n_estimators=500, max_depth=15,\n",
    "        min_samples_split=5, min_samples_leaf=2,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf_fold.fit(X_fold_train_s, y_fold_train)\n",
    "    oof_preds['RF'][val_idx] = rf_fold.predict(X_fold_val_s)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_fold = xgb.XGBRegressor(\n",
    "        n_estimators=1000, max_depth=6,\n",
    "        learning_rate=0.1, subsample=0.8,\n",
    "        colsample_bytree=0.8, random_state=42,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    xgb_fold.fit(X_fold_train_s, y_fold_train,\n",
    "                 eval_set=[(X_fold_val_s, y_fold_val)],\n",
    "                 verbose=False)\n",
    "    oof_preds['XGBoost'][val_idx] = xgb_fold.predict(X_fold_val_s)\n",
    "\n",
    "print(\"\\n✓ Out-of-fold predictions generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a7613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: TRAINING META-MODEL ON OUT-OF-FOLD PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Meta-model trained!\n",
      "Ridge coefficients:\n",
      "  CNN            : -0.0480\n",
      "  DFNN           : 0.0009\n",
      "  RNN            : -0.0667\n",
      "  Random Forest  : 0.5436\n",
      "  XGBoost        : 0.5664\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: TRAINING META-MODEL ON OUT-OF-FOLD PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create feature matrix for meta-model\n",
    "X_meta = np.column_stack([\n",
    "    oof_preds['CNN'],\n",
    "    oof_preds['DFNN'],\n",
    "    oof_preds['RNN'],\n",
    "    oof_preds['RF'],\n",
    "    oof_preds['XGBoost']\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'CNN': cnn_fold,\n",
    "    'DFNN': dfnn_fold,\n",
    "    'RNN': rnn_fold,\n",
    "    'Random Forest': rf_fold,\n",
    "    'XGBoost': xgb_fold\n",
    "}\n",
    "\n",
    "# Train stacking model\n",
    "stacking_model = Ridge(alpha=1.0)\n",
    "stacking_model.fit(X_meta, y_train_val)\n",
    "\n",
    "print(\"\\nMeta-model trained!\")\n",
    "print(\"Ridge coefficients:\")\n",
    "for name, coef in zip(models, stacking_model.coef_):\n",
    "    print(f\"  {name:15}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57947f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: RETRAINING BASE MODELS ON ALL TRAIN+VAL DATA\n",
      "============================================================\n",
      "Retraining all models on full dataset...\n",
      "  CNN...\n",
      "  DFNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 03:01:10.832444: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_select_fusion', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:14.043000: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 03:01:14.043051: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 03:01:14.043083: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 03:01:15.022277: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_722', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:15.456003: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_722', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:15.582333: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_722', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:17.252629: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1901', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:17.561281: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1808', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:17.795893: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1901', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:18.498882: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1901', 100 bytes spill stores, 100 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:20.787406: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-01 03:01:21.638307: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:22.143011: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-12-01 03:01:22.466053: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RNN...\n",
      "  Random Forest...\n",
      "  XGBoost...\n",
      "\n",
      "✓ All models retrained on full dataset!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: RETRAINING BASE MODELS ON ALL TRAIN+VAL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale all data\n",
    "scaler_final = StandardScaler()\n",
    "X_train_val_s = scaler_final.fit_transform(X_train_val)\n",
    "X_test_s = scaler_final.transform(X_test)\n",
    "\n",
    "# Reshape\n",
    "X_train_val_cnn = X_train_val_s.reshape(-1, len(features), 1)\n",
    "X_test_cnn = X_test_s.reshape(-1, len(features), 1)\n",
    "X_train_val_rnn = X_train_val_s.reshape(-1, len(features), 1)\n",
    "X_test_rnn = X_test_s.reshape(-1, len(features), 1)\n",
    "\n",
    "print(\"Retraining all models on full dataset...\")\n",
    "\n",
    "# CNN\n",
    "print(\"  CNN...\")\n",
    "cnn_final = Sequential([\n",
    "    Input((len(features),1)),\n",
    "    Conv1D(64, kernel_size=2, padding='same', activation='relu',\n",
    "           kernel_regularizer=regularizers.L2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.25),\n",
    "    Conv1D(128, kernel_size=2, padding='same', activation='relu',\n",
    "           kernel_regularizer=regularizers.L2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.L2(0.002)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.L2(0.002)),\n",
    "    BatchNormalization(), \n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn_final.compile(optimizer='adam', loss='mse')\n",
    "cnn_final.fit(X_train_val_cnn, y_train_val,\n",
    "              validation_split=0.1, epochs=100, batch_size=32,\n",
    "              callbacks=[es, rlr], verbose=0)\n",
    "\n",
    "# DFNN\n",
    "print(\"  DFNN...\")\n",
    "dfnn_final = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(len(features),)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "dfnn_final.compile(optimizer='adam', loss='mse')\n",
    "dfnn_final.fit(X_train_val_s, y_train_val,\n",
    "               validation_split=0.1, epochs=100, batch_size=64,\n",
    "               callbacks=[es, rlr], verbose=0)\n",
    "\n",
    "# RNN\n",
    "print(\"  RNN...\")\n",
    "rnn_final = Sequential([\n",
    "    Input((len(features), 1)),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "rnn_final.compile(optimizer='adam', loss='mse')\n",
    "rnn_final.fit(X_train_val_rnn, y_train_val,\n",
    "              validation_split=0.1, epochs=130, batch_size=16,\n",
    "              callbacks=[es, rlr], verbose=0)\n",
    "\n",
    "# RF\n",
    "print(\"  Random Forest...\")\n",
    "rf_final = RandomForestRegressor(\n",
    "    n_estimators=500, max_depth=15,\n",
    "    min_samples_split=5, min_samples_leaf=2,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_final.fit(X_train_val_s, y_train_val)\n",
    "\n",
    "# XGBoost\n",
    "print(\"  XGBoost...\")\n",
    "xgb_final = xgb.XGBRegressor(\n",
    "    n_estimators=1000, max_depth=6,\n",
    "    learning_rate=0.1, subsample=0.8,\n",
    "    colsample_bytree=0.8, random_state=42\n",
    ")\n",
    "xgb_final.fit(X_train_val_s, y_train_val, verbose=False)\n",
    "\n",
    "print(\"\\n✓ All models retrained on full dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41781bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: FINAL EVALUATION ON TEST SET\n",
      "============================================================\n",
      "\n",
      "Individual model results:\n",
      "CNN: R²=0.8938, RMSE=610.65, MAE=448.71, RelRMSE=6.61%\n",
      "DFNN: R²=0.7852, RMSE=868.28, MAE=629.59, RelRMSE=9.40%\n",
      "RNN: R²=0.8835, RMSE=639.61, MAE=459.56, RelRMSE=6.92%\n",
      "Random Forest: R²=0.8578, RMSE=706.44, MAE=544.34, RelRMSE=7.64%\n",
      "XGBoost: R²=0.8368, RMSE=756.94, MAE=563.73, RelRMSE=8.19%\n",
      "\n",
      "Ensemble results:\n",
      "Simple Average: R²=0.8894, RMSE=623.04, MAE=463.02, RelRMSE=6.74%\n",
      "Stacking (CV-based, PROPER): R²=0.8490, RMSE=728.05, MAE=557.82, RelRMSE=7.88%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate(name, y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rel = rmse/(y_true.max()-y_true.min())*100\n",
    "    print(f\"{name}: R²={r2:.4f}, RMSE={rmse:.2f}, MAE={mae:.2f}, RelRMSE={rel:.2f}%\")\n",
    "\n",
    "y_test_pred_cnn = cnn_final.predict(X_test_cnn, verbose=0).flatten()\n",
    "y_test_pred_dfnn = dfnn_final.predict(X_test_s, verbose=0).flatten()\n",
    "y_test_pred_rnn = rnn_final.predict(X_test_rnn, verbose=0).flatten()\n",
    "y_test_pred_rf = rf_final.predict(X_test_s)\n",
    "y_test_pred_xgb = xgb_final.predict(X_test_s)\n",
    "\n",
    "print(\"\\nIndividual model results:\")\n",
    "evaluate(\"CNN\", y_test, y_test_pred_cnn)\n",
    "evaluate(\"DFNN\", y_test, y_test_pred_dfnn)\n",
    "evaluate(\"RNN\", y_test, y_test_pred_rnn)\n",
    "evaluate(\"Random Forest\", y_test, y_test_pred_rf)\n",
    "evaluate(\"XGBoost\", y_test, y_test_pred_xgb)\n",
    "\n",
    "# Stacking prediction\n",
    "X_test_meta = np.column_stack([\n",
    "    y_test_pred_cnn,\n",
    "    y_test_pred_dfnn,\n",
    "    y_test_pred_rnn,\n",
    "    y_test_pred_rf,\n",
    "    y_test_pred_xgb\n",
    "])\n",
    "y_test_pred_stacking = stacking_model.predict(X_test_meta)\n",
    "\n",
    "print(\"\\nEnsemble results:\")\n",
    "y_test_pred_simple = np.mean(X_test_meta, axis=1)\n",
    "evaluate(\"Simple Average\", y_test, y_test_pred_simple)\n",
    "evaluate(\"Stacking (CV-based, PROPER)\", y_test, y_test_pred_stacking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFlow",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
